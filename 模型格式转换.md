
# safetensors格式 转换 gguff格式，并导入Ollama

## 1. 克隆llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
```

## 2. 构建虚拟环境

```bash
conda create llamacpp python=3.10
```

## 3. 安装llama.cpp依赖环境

C++和Python环境二选其一即可

### 1) 编译C++环境

量化需要

```bash
cd llama.cpp
mkdir build
cd build
cmake ..
make -j
```

### 2）安装Python环境

模型转换需要

```bash
conda activate llamacpp
pip install -r requirements.txt
```

## 3. 模型转换

### Python方式

从safetensors格式转换为gguf格式（Q8_0）

```bash
cd llama.cpp
python convert_hf_to_gguf.py ../your_model_path \
 --outfile /your_output_path/model.gguf \
 --outtype q8_0
```

写了个转换脚本，从safetensors转为bin格式，似乎用不上。

```bash
python convert.py
```
### C++方式

转换 safetensors 模型为 Ollama 格式(F16)
```bash
python ../../convert_hf_to_gguf.py ../qwen2-vl-2b --outtype f16
```

量化
注意，这个需要编译llama.cpp后在build/bin路径下

```bash
llama.cpp/build/bin/llama-quantize ../qwen2-vl-2b/qwen2-vl-2B-F16.gguf q8_0
```

或者
```bash
llama.cpp/build/bin/llama-quantize ../qwen2-vl-2b/qwen2-vl-2B-F16.gguf q4_0
```

## 4. 模型导入

创建Modelfile

```bash
echo '/path/to/your_model.gguf' > Modelflie
```

创建（导入）模型

```bash
ollama create model_name -f /path/to/Modelfile
```

```text
(llamacpp) jetson@jetson-orin-nx-super:~/qwen2-vl-2b$ ollama create qwen2-vl-2b -f Modelfile
gathering model components
copying file sha256:1709aa285974b03259b129f2d5bd819beee9a44a0c3d79ca82291fe41838e3d7 100%
parsing GGUF
using existing layer sha256:1709aa285974b03259b129f2d5bd819beee9a44a0c3d79ca82291fe41838e3d7
writing manifest
success
```

查看模型

```bash
ollama list
```

如下：

```text
(llamacpp) jetson@jetson-orin-nx-super:~/qwen2-vl-2b$ ollama list
NAME                  ID              SIZE      MODIFIED
qwen2-vl-2b:latest    41a2703169aa    1.8 GB    20 seconds ago
```
